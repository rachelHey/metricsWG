# A list of other initiatives

In the last decades many initiatives started to change the evaluation and assessment process in research, for funding allocation or hiring at universities. Such initiatives are facilitated by the creation or implementation of new metrics to measure or assess Open Science Practices, Reproducibility, Transparency, Reporting, and other Good Research Practices. The aim of the initiatives is to move away from traditional metrics based on scientific publication, e.g. number of publications, citations, publications in high impact journals. 

Down here, a non-exhaustive list of initiatives (other than COARA) - curated by Zhixuan Li (Research assistant at EBPI, University of Zurich).

**Pathos**

The EU project "Open Science Impact Pathways (Pathos)" published an Open Science Indicator Handbook: [handbook.pathos-project.eu](https://handbook.pathos-project.eu/).
The project was interested in the causal effect of Open Science on the impact of the science. The indicator handbook provides guidance on how to operationalise various indicators in order to facilitate studies on the effect of Open Science. Within the PathoS project the indicator handbook will remain in continuous development until January 2025.
 
<details>
<summary> <i><u>Indicators in the Pathos OS indicator handbook (five categories)</u></i> </summary>

1. Open science:
 
   - APC Costs
   - Availability of data repositories
   - Availability of preprint repositories
   - Availability of publication repositories
   - Citizen Science Indicators
   - Deposition of Open Metadata
   - Evaluation of Open Science in research assessment
   - Distribution of Open Access journal models
   - Prevalence of Open Access publishing
   - Open Science training facilities
   - Prevalence of national Open Science policies
   - Prevalence of open/FAIR data practices
   - Prevalence of Open Method practices
   - Prevalence of Open Peer Review
   - Prevalence of Open Science funding policies
   - Prevalence of Open Science support
   - Prevalence of preprinting
   - Prevalence of replication studies
   - Transformative publishing agreements
   
2. Academic Impact:
 
   - Academic readership
   - Citation impact
   - Collaboration intensity
   - Diversity
   - Industry collaboration
   - Interdisciplinarity
   - Novelty
   - Societal collaboration
   - Topic trends
   - Use of code in research
   - Use of data in research
   - Use of methods in research
   - Use of patents in research
   
3. Societal Impact:
 
   - Uptake in citizen science
   - Uptake in education
   - Science literacy
   - Uptake by policy makers
   - Uptake in the legal sector
   - Uptake in the public debate
   - Uptake in medical practice
   - Uptake by patient groups
   - Uptake by civil society
   - Uptake by the general public
   - Uptake by artistic sector
   - Effect on democracy
   - Effect on ethnic inequality
   - Effect on gender inequality
   - Effect on SDGs
   
4. Economic Impact:
 
   - Science-industry collaboration
   - Innovation output
   - Uptake of research result by industry
   - Socially relevant products and processes
   - Economic growth of companies
   - Labour market impacts
   - Cost savings
   
5. Reproducibility:
 
   - Consistency in reported numbers
   - Impact of Open Code in research
   - Impact of Open Data in research
   - Inclusion in systematic reviews or meta-analyses
   - Level of replication
   - Polarity of publications
   - Reuse of code in research
   - Reuse of data in research
 
</details>

<br>

**OPUS**

The EU project "Open and Universal Science Project" works on indicators and metrics for OS, with an already available [deliverable report](https://opusproject.eu/wp-content/uploads/2023/09/OPUS_D3.1_IndicatorsMetrics_FINAL_PUBLIC.pdf).
<details><summary> <i><u>Main messages of the summary</u></i> </summary>
- The report proposes a first draft of a researcher assessment framework (RAF) to assess researchers in an academic context.
 - This first draft of the RAF will be further developed in iterations, including testing and feedback from the pilots, targeted feedback from key stakeholders, and an open consultation with the wider research community.
 - The RAF consists of 4 main categories for research, education, leadership, and valorisation activities, which are further subdivided into relevant subcategories consisting of specific sets of indicators and metrics. The RAF furthermore consists of 2 main dimensions for generic and Open Science activities, whereby the indicators remain the same for both categories, but the metrics are either generic or specifically focused on Open Science.
 - The report begins with the guiding principles behind the RAF and describes the overall structure and implementation of the RAF at RPOs and RFOs in Section 2. 
 - The report next presents the generic RAF and lists the generic indicators and metrics for researcher assessment at RPOs and RFOs in Section 3. 
 - The report then presents the Open Science RAF and lists the indicators and metrics to recognise and reward Open Science practices in researcher assessment at RPOs and RFOs in Section 4. 
 - The report lastly offers conclusions and next steps for OPUS in Section 5 and provides **the full RAF with all generic and Open Science indicators and metrics in table format in Appendix 1**.
</details>

<br>

**GRASPOS**

The EU project works an "Open research assessment dataspace", [graspos.eu/](https://graspos.eu/). The goal is to develop, assess and operate an open and trusted federated infrastructure for next generation research metrics and indicators, offering data, tools, services and guidance. 

<details>
<summary> <i><u>More specifically</u></i> </summary>

 - _Open and Federated Research Assessment Infrastructure_: GraspOS aims to address the challenges of Open-Science-aware Responsible Research Assessment (OS-aware RRA) by designing and delivering an Open and Federated Research Assessment Infrastructure (formerly known as “Federated Open Metrics Infrastructure - FOMI”). This infrastructure paves the way for the creation of an Open Research Assessment Dataspace (ORAD) by **aggregating open resources** (e.g., next-generation metrics and indicators, data, tools, services, and guidance offered by different sources) that can catalyse the implementation of policy reforms towards an OS-aware RRA framework. The value of this infrastructure will be evaluated and showcased in practice by the GraspOS piloting activities (WP5) at different levels of granularity: from  researcher (individual/group) and institutional level to organisational and country level. 
 
 - _Open Science Assessment Framework (OSAF):_ A co-developed Open Science Assessment Framework (OSAF) will document and provide options on how indicators and tools used in a particular evaluation setting align with the specific characteristics of the assessment context, such as the disciplinary and institutional context and the level at which the assessment takes place (e.g., individual researchers, research groups, research institutions, national level). It will be a key resource for advising implementation and next steps. See [diagram](https://graspos.eu/open-science-assessment-framework)
 
 - _Assessment Portfolios:_ Offering mix-and-match capabilities and guidance, GraspOS will develop different (pre-set) templates that fit various needs of R&I actors, i.e., funders, institutions, research teams, and disciplines, providing a flexible way for them to adopt openness that fits their own needs and pace. GraspOS will provide customisable Dashboard Services which will manifest the Openness profile, by automatically collecting data for the indicators, and allowing manual annotation capabilities for end users to describe strategies and achieved impacts also by capturing additional outputs (narrative CV). GraspOS will develop RRA templates that can be used as fit-for-purpose templates for collecting and structuring both quantitative and qualitative indicators and constiture 
 
 - _Assessment Registry:_ Building on DORA efforts (e.g., Project TARA) GraspOS will build the Assessment Registry. This will be a service that captures information about Open Science assessments, related protocols and indicator toolboxes in a structured format, namely the characteristics of the context, the design of the indicator toolbox, as well as lessons learned.
 
 - _Enrichment services:_ GraspOS will build upon existing technologies provided by its partners to deliver EOSC-native tools and services and deliver metrics taking into consideration the requirements of the pilots and recommendations from the “Open Science Policy Platform Recommendations' the “Indicator Frameworks for Fostering Open Knowledge Practices in Science and Scholarship” and the RDA Interest Group on “Open Science Graphs for FAIR Data” .
 
 - _Monitoring Services:_ GraspOS will offer a set of practical instruments that support monitoring the usage, uptake, quality, and impact of research outputs, researchers, research infrastructures, and OS practices, affecting the way relevant actors (individual researchers, group of researchers, institutions, infrastructures) can be assessed according to OS-aware RRA approaches.
 
 - _Training Material:_ (aimed at educating stakeholders on the use of indicators, tools, services and infrastructure to further assist their use in the context of implementing Open Science-aware Responsible Research Aassessment approaches.) A training course on the Open Science Assessment Framework will be co-developed with the technology and infrastructure providers and hosted on the platform www.openplato.eu, a complete learning platform offered by OpenAIRE for organising all the activities, as well as on other similar platforms that may be available (e.g. via EOSC). Additional training events will focus on the topics of the Open Metrics Infrastructure. 
 
</details>

<br>

**LERU (Next generation metrics)**

 - Survey use of bibliometrics as part of assessment practice in universities
 - Identify university policy positions on use of bibliometrics as part of assessment practices
 - Formulate advice paper/statement on the role/non-role of alternative metrics to guide best practice


<br>

**SciLake**

[Scilake](https://scilake.eu/) will deliver a range of diverse outputs including recommendations for policy makers, a handbook of OS indicators, recommendations for metrics, other tools and data, and methodology.

<details>
<summary> <i><u>More specifically</u></i> </summary>
 - _Recommendations for policy makers: _ Recommendations for policy makers and implementors of Open Science on how and where to intervene: actions for efficient implementation and improvement based on set objectives. 
 
 - _Handbook of Open Science Impact Indicators:_ A Handbook of Open Science Impact Indicators mapped to the three impact areas, accompanied by a set of methods and proposed data/data sources. 
 
 - _Recommendations for metrics:_ Recommendations on how to develop/improve the underlying metrics infrastructure. 
 
 - _Tools & data:_ A set of tools and data that measure Open Science impact indicators (code, data, toolkits). 
 - _Methodology:_ 
    - TIER I: Offer a comprehensive, open, transparent, and customisable scientific data-lake-as-a-service, empowering and facilitating the creation, interlinking, and maintenance of SKGs both across and within different scientific disciplines.
    - TIER II: Build and offer a set of customisable, AI-assisted services that facilitate the navigation of scholarly content following a scientific merit-driven approach, focusing on two merit aspects which are crucial for the research community at large: impact and reproducibility.

</details>

<br>

**QuOCCA**

The Research Quality Committee at Neuroscience Research Australia (NeuRA) has developed a new tool to assess and improve scientific research quality and reproducibility: Quality Output Checklist and Content Assessment (QuOCCA) The QuOCCA was designed after extensive research and feedback from external experts and researchers. The QuOCCA was specifically created to assess a broad spectrum of biomedical research papers. However, considering its generic nature and wide coverage, many of the components could also be extended to assess published work from other disciplines as well. Assess published work [here](https://neura.edu.au/about/research-quality).

<br> 

**SPACE**


The [SPACE rubric](https://sfdora.org/resource/space-to-evolve-academic-assessment-a-rubric-for-analyzing-institutional-conditions-and-progress-indicators/) (also [here](https://sfdora.org/resource-library/?_resource_type=tools&_dora_produced=1)) was developed to help institutions at any stage of academic assessment reform gauge their institutional ability to support interventions and set them up for success.

Organizations can use the SPACE rubric to support the implementation of fair and responsible academic career assessment practices in two ways: First, it can help establish a baseline for the current state of infrastructural conditions, to gauge an institution’s ability to support the development and implementation of new academic assessment practices and activities. Second, the rubric can be used to retroactively analyze how strengths or gaps in these institutional conditions may have impacted the outcomes of concrete interventions targeted to specific types of academic assessment activities - such as hiring, promotion, tenure, or even graduate student evaluation - either helping or hindering progress toward those goals.


**More?**

https://eua.eu/component/tags/tag/85-academic-career-assessment.html





